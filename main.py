import requests
import time
from datetime import datetime
import csv
import re

# Headers chung ƒë·ªÉ g·ª≠i request
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
    "Accept": "application/json",
    "Referer": "https://tiki.vn/",
}

# H√†m tr√≠ch xu·∫•t product_id v√† spid t·ª´ URL s·∫£n ph·∫©m
def extract_tiki_ids(url):
    match = re.search(r"p(\d+)(?:.+spid=(\d+))?", url)
    product_id = match.group(1) if match else None
    spid = match.group(2) if match and match.group(2) else None
    return product_id, spid

# H√†m l·∫•y t√™n s·∫£n ph·∫©m
def get_product_name(product_id):
    api_url = f"https://tiki.vn/api/v2/products/{product_id}?platform=web&version=3"
    response = requests.get(api_url, headers=headers)
    if response.status_code == 200:
        data = response.json()
        return data.get("name", "Unknown Product")
    return "Unknown Product"

# H√†m l·∫•y `spid` n·∫øu kh√¥ng c√≥ trong URL
def get_spid_if_missing(product_id):
    api_url = f"https://tiki.vn/api/v2/reviews?limit=1&page=1&product_id={product_id}"
    response = requests.get(api_url, headers=headers)

    if response.status_code == 200:
        data = response.json()
        first_review = data.get("data", [{}])[0]
        return first_review.get("spid")
    return None

# H√†m l·∫•y seller_id t·ª´ API reviews
def get_seller_id(product_id, spid):
    api_url = f"https://tiki.vn/api/v2/reviews?limit=1&page=1&spid={spid}&product_id={product_id}"
    response = requests.get(api_url, headers=headers)
    if response.status_code == 200:
        data = response.json()
        seller_id = data.get("data", [{}])[0].get("seller_id", 1)
        return seller_id
    return 1  # M·∫∑c ƒë·ªãnh l√† 1 n·∫øu kh√¥ng l·∫•y ƒë∆∞·ª£c

# H√†m l·∫•y t·ªïng s·ªë trang reviews
def get_total_pages(product_id, spid, seller_id, limit=20):
    api_url = f"https://tiki.vn/api/v2/reviews?limit={limit}&page=1&spid={spid}&product_id={product_id}&seller_id={seller_id}"
    response = requests.get(api_url, headers=headers)
    if response.status_code == 200:
        data = response.json()
        total_reviews = data.get("paging", {}).get("total", 0)
        last_page = data.get("paging", {}).get("last_page", 1)
        if total_reviews == 0:
            print("S·∫£n ph·∫©m n√†y kh√¥ng c√≥ ƒë√°nh gi√° n√†o!")
            return 0
        print(f"T·ªïng s·ªë ƒë√°nh gi√°: {total_reviews}, S·ªë trang c·∫ßn l·∫•y: {last_page}")
        return last_page
    else:
        print(f"L·ªói khi l·∫•y t·ªïng s·ªë trang: {response.status_code}")
        return 0

# H√†m crawl review t·ª´ trang 1 ƒë·∫øn last_page
def crawl_reviews(product_id, spid, seller_id, output_file, limit=20):
    last_page = get_total_pages(product_id, spid, seller_id, limit)
    if last_page == 0:
        print("Kh√¥ng c√≥ ƒë√°nh gi√° ƒë·ªÉ l∆∞u!")
        return

    all_reviews = []
    for page in range(1, last_page + 1):
        api_url = f"https://tiki.vn/api/v2/reviews?limit={limit}&page={page}&spid={spid}&product_id={product_id}&seller_id={seller_id}"
        response = requests.get(api_url, headers=headers)

        if response.status_code == 200:
            data = response.json()
            reviews = data.get("data", [])
            all_reviews.extend(reviews)
            print(f"ƒê√£ l·∫•y {len(reviews)} review t·ª´ trang {page}/{last_page}")
        else:
            print(f"L·ªói khi l·∫•y trang {page}: {response.status_code}")

        time.sleep(1)  # Tr√°nh b·ªã ch·∫∑n

    # L∆∞u v√†o CSV
    save_reviews_to_csv(all_reviews, output_file)
    print(f"T·ªïng s·ªë review ƒë√£ l∆∞u: {len(all_reviews)}")

# H√†m l∆∞u d·ªØ li·ªáu v√†o file CSV
def save_reviews_to_csv(reviews, filename):
    with open(filename, mode="w", newline="", encoding="utf-8-sig") as file:
        writer = csv.writer(file)
        writer.writerow(["review_id", "rating", "content", "created_at"])
        for review in reviews:
            timestamp = review.get("created_at")
            formatted_date = datetime.utcfromtimestamp(timestamp).strftime("%Y-%m-%d %H:%M:%S") if timestamp else "N/A"
            writer.writerow([
                review.get("id"),
                review.get("rating"),
                review.get("content"),
                formatted_date,
            ])
    print(f"D·ªØ li·ªáu ƒë√£ l∆∞u v√†o {filename}")

# Ch·∫°y ch∆∞∆°ng tr√¨nh
if __name__ == "__main__":
    product_url = input("üîó Nh·∫≠p link s·∫£n ph·∫©m Tiki: ").strip()
    product_id, spid = extract_tiki_ids(product_url)

    if not product_id:
        print("Kh√¥ng t√¨m th·∫•y product_id trong URL!")
    else:
        product_name = get_product_name(product_id)
        print(f"T√™n s·∫£n ph·∫©m: {product_name}")

        if not spid:
            print("Kh√¥ng t√¨m th·∫•y spid trong URL, ƒëang t√¨m ki·∫øm tr√™n API...")
            spid = get_spid_if_missing(product_id)

        if not spid:
            print("Kh√¥ng l·∫•y ƒë∆∞·ª£c spid, kh√¥ng th·ªÉ ti·∫øp t·ª•c!")
        else:
            seller_id = get_seller_id(product_id, spid)
            print(f"T·ª± ƒë·ªông l·∫•y: product_id={product_id}, spid={spid}, seller_id={seller_id}")

            output_file = f"reviews_{product_id}.csv"
            crawl_reviews(product_id, spid, seller_id, output_file)
